{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p> Machine Learning Serving focused on GenAI &amp; LLMs with simplicity as the top priority. </p> <p> Docs |   Examples </p> <p> </p>"},{"location":"#installation","title":"Installation","text":"<p>Stable: <pre><code>pip install FastServeAI\n</code></pre></p> <p>Latest: <pre><code>pip install git+https://github.com/gradsflow/fastserve-ai.git@main\n</code></pre></p>"},{"location":"#usageexamples","title":"Usage/Examples","text":"<p>YouTube: How to serve your own GPT like LLM in 1 minute with FastServe.</p>"},{"location":"#serve-custom-model","title":"Serve Custom Model","text":"<p>To serve a custom model, you will have to implement <code>handle</code> method for <code>FastServe</code> that processes a batch of inputs and returns the response as a list.</p> <pre><code>from fastserve import FastServe\n\n\nclass MyModelServing(FastServe):\n    def __init__(self):\n        super().__init__(batch_size=2, timeout=0.1)\n        self.model = create_model(...)\n\n    def handle(self, batch: List[BaseRequest]) -&gt; List[float]:\n        inputs = [b.request for b in batch]\n        response = self.model(inputs)\n        return response\n\n\napp = MyModelServing()\napp.run_server()\n</code></pre> <p>You can run the above script in terminal, and it will launch a FastAPI server for your custom model.</p>"},{"location":"#deploy","title":"Deploy","text":""},{"location":"#lightning-ai-studio","title":"Lightning AI Studio \u26a1\ufe0f","text":"<pre><code>python fastserve.deploy.lightning --filename main.py \\\n    --user LIGHTNING_USERNAME \\\n    --teamspace LIGHTNING_TEAMSPACE \\\n    --machine \"CPU\"  # T4, A10G or A10G_X_4\n</code></pre>"},{"location":"#contribute","title":"Contribute","text":"<p>Install in editable mode:</p> <pre><code>git clone https://github.com/gradsflow/fastserve-ai.git\ncd fastserve\npip install -e .\n</code></pre> <p>Create a new branch</p> <pre><code>git checkout -b \uff1cnew-branch\uff1e\n</code></pre> <p>Make your changes, commit and create a PR.</p>"},{"location":"404/","title":"Oops! The page you are looking for does not exist.","text":""},{"location":"CHANGELOG/","title":"Release Notes","text":""},{"location":"CHANGELOG/#003","title":"0.0.3","text":"<ul> <li>Setup repo</li> <li>Feat/refactor api by @aniketmaurya in https://github.com/aniketmaurya/fastserve-ai/pull/8</li> <li>deploy lightning by @aniketmaurya in https://github.com/aniketmaurya/fastserve-ai/pull/9</li> <li>add face recognition by @aniketmaurya in https://github.com/aniketmaurya/fastserve-ai/pull/10</li> <li>add image classification by @aniketmaurya in https://github.com/aniketmaurya/fastserve-ai/pull/11</li> <li>document youtube video by @aniketmaurya in https://github.com/aniketmaurya/fastserve-ai/pull/12</li> <li>[pre-commit.ci] pre-commit suggestions by @pre-commit-ci in https://github.com/aniketmaurya/fastserve-ai/pull/13</li> <li>Serve UI by @aniketmaurya in https://github.com/aniketmaurya/fastserve-ai/pull/14</li> <li>Redirect <code>/ui</code> to web app by @aniketmaurya in https://github.com/aniketmaurya/fastserve-ai/pull/15</li> <li>add vLLM by @aniketmaurya in https://github.com/aniketmaurya/fastserve-ai/pull/21</li> </ul>"},{"location":"css/","title":"Css","text":"<pre><code>&lt;style&gt;\n:root {\n--brand-color: #FF6A51;\n--darkest-color: #29323E;\n--dark-color: #3A4655;\n--mid-color: #C4BC9D;\n--light-color: #F1EEE2;\n--lightest-color: #FFFFFF;\n  }\n&lt;/style&gt;\n</code></pre> <pre><code>// Add this to tailwind.config.js\nconst colors = require(\"tailwindcss/colors\");\n\nmodule.exports = {\n  theme: {\n    extend : {\n      colors: {\n        brand: \"#FF6A51\",\n        darkest: \"#29323E\",\n        dark: \"#3A4655\",\n        mid: \"#C4BC9D\",\n        light: \"#F1EEE2\",\n        lightest: \"#FFFFFF\"\n      },\n    },\n  },\n};\n</code></pre>"},{"location":"fastserve/containerization/","title":"Run and deploy with Docker container \ud83d\udc33","text":""},{"location":"fastserve/containerization/#containerization","title":"Containerization","text":"<p>To containerize your FastServe application, a Docker example is provided in the examples/docker-compose-example directory. The example is about face recognition and includes a <code>Dockerfile</code> for creating a Docker image and a <code>docker-compose.yml</code> for easy deployment. Here's a quick overview:</p> <ul> <li>Dockerfile: Defines the environment, installs dependencies from <code>requirements.txt</code>, and specifies the command to run your FastServe application.</li> <li>docker-compose.yml: Simplifies the deployment of your FastServe application by defining services, networks, and volumes.</li> </ul> <p>To use the example, navigate to the <code>examples/docker-compose-example</code> directory and run:</p> <pre><code>docker-compose up --build\n</code></pre> <p>This will build the Docker image and start your FastServe application in a container, making it accessible on the specified port.</p> <p>Note: We provide an example using face recognition. If you need to use other models, you will likely need to change the requirements.txt or the Dockerfile. Don't worry; this example is intended to serve as a quick start. Feel free to modify it as needed.</p>"},{"location":"fastserve/containerization/#passing-arguments-to-uvicorn-in-run_server","title":"Passing Arguments to Uvicorn in <code>run_server()</code>","text":"<p>FastServe leverages Uvicorn, a lightning-fast ASGI server, to serve machine learning models, making FastServe highly efficient and scalable. The <code>run_server()</code> method supports passing additional arguments to uvicorn by utilizing <code>*args</code> and <code>**kwargs</code>. This feature allows you to customize the server's behavior without modifying the source code. For example:</p> <pre><code>app.run_server(host='0.0.0.0', port=8000, log_level='info')\n</code></pre> <p>In this example, host, port, and log_level are passed directly to uvicorn.run() to specify the server's IP address, port, and logging level. You can pass any argument supported by <code>uvicorn.run()</code> to <code>run_server()</code> in this manner.</p>"},{"location":"fastserve/models/face_detection/","title":"Serve Face  Detection","text":"<pre><code>from fastserve.models import FaceDetection\n\nserve = FaceDetection(batch_size=2, timeout=1)\nserve.run_server()\n</code></pre> <p>or, run <code>python -m fastserve.models --model face-detection --batch_size 2 --timeout 1</code> from terminal.</p>"},{"location":"fastserve/models/image_classification/","title":"Serve Image Classification models with FastServe","text":""},{"location":"fastserve/models/image_classification/#image-classification","title":"Image Classification","text":"<pre><code>from fastserve.models import ServeImageClassification\n\napp = ServeImageClassification(\"resnet18\", timeout=1, batch_size=4)\napp.run_server()\n</code></pre> <p>or, run <code>python -m fastserve.models --model image-classification --model_name resnet18 --batch_size 4 --timeout 1</code> from terminal.</p>"},{"location":"fastserve/models/image_gen/","title":"Serve GenAI - Image Generation Models","text":""},{"location":"fastserve/models/image_gen/#serve-sdxl-turbo","title":"Serve SDXL Turbo","text":"<pre><code>from fastserve.models import ServeSDXLTurbo\n\nserve = ServeSDXLTurbo(device=\"cuda\", batch_size=2, timeout=1)\nserve.run_server()\n</code></pre> <p>or, run <code>python -m fastserve.models --model sdxl-turbo --batch_size 2 --timeout 1</code> from terminal.</p> <p>This application comes with an UI. You can access it at http://localhost:8000/ui .</p> <p></p>"},{"location":"fastserve/models/llms/hf/","title":"\ud83e\udd17 Hugging Face","text":""},{"location":"fastserve/models/llms/hf/#serve-huggingface-models","title":"Serve HuggingFace Models","text":"<p>Leveraging FastServe, you can seamlessly serve any HuggingFace Transformer model, enabling flexible deployment across various computing environments, from CPU-based systems to powerful GPU and multi-GPU setups.</p> <p>For some models, it is required to have a HuggingFace API token correctly set up in your environment to access models from the HuggingFace Hub. This is not necessary for all models, but you may encounter this requirement, such as accepting terms of use or any other necessary steps. Take a look at your model's page for specific requirements. <pre><code>export HUGGINGFACE_TOKEN=&lt;your hf token&gt;\n</code></pre></p> <p>The server can be easily initiated with a specific model. In the example below, we demonstrate using <code>gpt2</code>. You should replace <code>gpt2</code> with your model of choice. The <code>model_name</code> parameter is optional; if not provided, the class attempts to fetch the model name from an environment variable <code>HUGGINGFACE_MODEL_NAME</code>. Additionally, you can now specify whether to use GPU acceleration with the <code>device</code> parameter, which defaults to <code>cpu</code> for CPU usage.</p> <pre><code>from fastserve.models import ServeHuggingFace\n\n# Initialize with GPU support if desired by setting `device=\"cuda\"`.\n# For CPU usage, you can omit `device` or set it to `cpu`.\napp = ServeHuggingFace(model_name=\"gpt2\", device=\"cuda\")\napp.run_server()\n</code></pre> <p>or, run <code>python -m fastserve.models --model huggingface --model_name bigcode/starcoder --batch_size 4 --timeout 1 --device cuda</code> from terminal.</p> <p>To make a request to the server, send a JSON payload with the prompt you want the model to generate text for. Here's an example using requests in Python: <pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:8000/endpoint\",\n    json={\"prompt\": \"Once upon a time\", \"temperature\": 0.7, \"max_tokens\": 100}\n)\nprint(response.json())\n</code></pre> This setup allows you to easily deploy and interact with any Transformer model from HuggingFace's model hub, providing a convenient way to integrate AI capabilities into your applications.</p> <p>Remember, for deploying specific models, ensure that you have the necessary dependencies installed and the model files accessible if they are not directly available from HuggingFace's model hub.</p>"},{"location":"fastserve/models/llms/local_llms/","title":"Serve LLMs locally","text":""},{"location":"fastserve/models/llms/local_llms/#serve-llms-with-llama-cpp","title":"Serve LLMs with Llama-cpp","text":"<pre><code>from fastserve.models import ServeLlamaCpp\n\nmodel_path = \"openhermes-2-mistral-7b.Q5_K_M.gguf\"\nserve = ServeLlamaCpp(model_path=model_path, )\nserve.run_server()\n</code></pre> <p>or, run <code>python -m fastserve.models --model llama-cpp --model_path openhermes-2-mistral-7b.Q5_K_M.gguf</code> from terminal.</p>"},{"location":"fastserve/models/llms/vllm/","title":"Serve LLMs at Scale with vLLM","text":""},{"location":"fastserve/models/llms/vllm/#serve-vllm","title":"Serve vLLM","text":"<pre><code>from fastserve.models import ServeVLLM\n\napp = ServeVLLM(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\napp.run_server()\n</code></pre> <p>You can use the FastServe client that will automatically apply chat template for you -</p> <pre><code>from fastserve.client import vLLMClient\nfrom rich import print\n\nclient = vLLMClient(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\nresponse = client.chat(\"Write a python function to resize image to 224x224\", keep_context=True)\n# print(client.context)\nprint(response[\"outputs\"][0][\"text\"])\n</code></pre>"}]}